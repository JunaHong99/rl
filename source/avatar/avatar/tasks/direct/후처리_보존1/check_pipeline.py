# # check_pipeline.py

# import torch
# import numpy as np
# from torch_geometric.data import Batch

# # --- 1. í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤. ---
# # (íŒŒì¼ ì´ë¦„ì´ ì‹¤ì œì™€ ë‹¤ë¥¸ ê²½ìš°, ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.)
# try:
#     from graph_converter import (
#         convert_state_to_graph,
#         NODE_FEATURE_DIM, EDGE_FEATURE_DIM, GLOBAL_FEATURE_DIM,
#         RAW_ROBOT_DIM, RAW_TASK_DIM, RAW_OBSTACLE_DIM
#     )
#     from replay_buffer import GraphReplayBuffer
#     from agent import TD3 as RoboBallet_TD3
# except ImportError as e:
#     print(f"Import Error: {e}")
#     print("ìŠ¤í¬ë¦½íŠ¸ ì„í¬íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„ê³¼ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
#     print("ì˜ˆìƒ íŒŒì¼: graph_converter.py, replay_buffer.py, agent.py")
#     exit()

# # --- 2. ì˜ˆì¸¡ ê°€ëŠ¥í•œ Mock Environment ---
# # main.pyì˜ MockEnvë¥¼ ê°œì„ í•˜ì—¬, í–‰ë™ì— ë”°ë¼ ìƒíƒœê°€ ë³€í•˜ê³  ëª…í™•í•œ ë³´ìƒ ì²´ê³„ë¥¼ ê°–ë„ë¡ í•©ë‹ˆë‹¤.
# class MockEnv:
#     def __init__(self, n_robots=2, n_tasks=1, max_steps=50, action_dim=7):
#         self.n_robots = n_robots
#         self.n_tasks = n_tasks
#         self.action_dim = action_dim
#         self.step_count = 0
#         self.max_steps = max_steps
#         self.tolerance = 0.05 # ëª©í‘œ ë„ë‹¬ í—ˆìš© ì˜¤ì°¨

#         # ê°€ìƒ í™˜ê²½ì˜ ìƒíƒœ
#         self.robot_state = torch.zeros(self.n_robots, RAW_ROBOT_DIM)
#         self.task_state = torch.zeros(self.n_tasks, RAW_TASK_DIM)
#         self.obstacle_state = torch.zeros(1, RAW_OBSTACLE_DIM)
#         self.goal_pos = torch.zeros(3)

#     def _get_raw_state(self):
#         return {
#             'robots': self.robot_state.clone(),
#             'tasks': self.task_state.clone(),
#             'obstacles': self.obstacle_state.clone(),
#             'globals': torch.tensor([self.step_count / self.max_steps, 0.0])
#         }

#     def reset(self):
#         self.step_count = 0
#         self.robot_state.zero_()
#         # ë¡œë´‡ì˜ ìœ„ì¹˜(15:18)ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„¤ì •
#         self.robot_state[:, 15:18] = torch.rand(self.n_robots, 3) * 2 - 1
        
#         # ëª©í‘œ(íƒœìŠ¤í¬) ìœ„ì¹˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„¤ì •
#         self.goal_pos = torch.rand(3) * 2 - 1
#         self.task_state[:, 0:3] = self.goal_pos
        
#         achieved_goal = self.robot_state[0, 15:18].clone()
#         info = {'achieved_goal': achieved_goal, 'desired_goal': self.goal_pos.clone()}
#         return self._get_raw_state(), info

#     def step(self, action: np.ndarray):
#         self.step_count += 1
        
#         # í–‰ë™(ì†ë„ ì œì–´)ì— ë”°ë¼ ë¡œë´‡ ìœ„ì¹˜ë¥¼ ì—…ë°ì´íŠ¸
#         action_tensor = torch.from_numpy(action).float()
#         # 7-dof ì¤‘ ì•ì˜ 3ê°œë§Œ ìœ„ì¹˜ ì œì–´ì— ì‚¬ìš©í•œë‹¤ê³  ê°€ì •
#         self.robot_state[:, 15:18] += action_tensor[:, 0:3] * 0.1 
        
#         next_raw_state = self._get_raw_state()
        
#         # ëª©í‘œ ë„ë‹¬ ì—¬ë¶€ í™•ì¸ (0ë²ˆ ë¡œë´‡ ê¸°ì¤€)
#         achieved_goal = self.robot_state[0, 15:18].clone()
#         distance = torch.norm(achieved_goal - self.goal_pos)
        
#         done = (distance < self.tolerance) or (self.step_count >= self.max_steps)
#         reward = 1.0 if (distance < self.tolerance) else 0.0
        
#         info = {'achieved_goal': achieved_goal, 'desired_goal': self.goal_pos.clone()}
        
#         return next_raw_state, reward, done, info

# # --- 3. íŒŒì´í”„ë¼ì¸ ê²€ì¦ ë©”ì¸ í•¨ìˆ˜ ---
# def run_pipeline_check():
#     print("--- RoboBallet ì½”ë“œ íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì‹œì‘ ---")

#     # --- ì„¤ì • ---
#     N_ROBOTS = 2
#     ACTION_DIM = 7
#     MAX_ACTION = 1.0
#     BATCH_SIZE = 4

#     # --- ë‹¨ê³„ 1: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ---
#     try:
#         env = MockEnv(n_robots=N_ROBOTS, action_dim=ACTION_DIM)
#         replay_buffer = GraphReplayBuffer(capacity=1000)
#         gnn_params = {
#             'node_dim': NODE_FEATURE_DIM, 'edge_dim': EDGE_FEATURE_DIM,
#             'global_dim': GLOBAL_FEATURE_DIM, 'action_dim': ACTION_DIM
#         }
#         agent = RoboBallet_TD3(gnn_params=gnn_params, max_action=MAX_ACTION)
#         print("âœ… [1/5] ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
#     except Exception as e:
#         print(f"âŒ [1/5] ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return

#     # --- ë‹¨ê³„ 2: í™˜ê²½ ë¦¬ì…‹ ë° ê·¸ë˜í”„ ë³€í™˜ ---
#     raw_state, _ = env.reset()
#     state_graph = convert_state_to_graph(raw_state)
#     print("âœ… [2/5] í™˜ê²½ ë¦¬ì…‹ ë° ê·¸ë˜í”„ ë³€í™˜ ì„±ê³µ")
#     print(f"    - ë³€í™˜ëœ State Graph: {state_graph}")
#     print(f"    - ë…¸ë“œ ìˆ˜: {state_graph.num_nodes}, ì—£ì§€ ìˆ˜: {state_graph.num_edges}")

#     # --- ë‹¨ê³„ 3: í–‰ë™ ì„ íƒ (Action Selection) ---
#     action_tensor = agent.select_action(state_graph)
#     robot_actions = action_tensor[0:N_ROBOTS]
#     print("âœ… [3/5] ì—ì´ì „íŠ¸ í–‰ë™ ì„ íƒ ì„±ê³µ")
#     print(f"    - Actorê°€ ì¶œë ¥í•œ ì „ì²´ Action Tensor Shape: {action_tensor.shape}")
#     print(f"    - í™˜ê²½ì— ì „ë‹¬ë  Robot Action Shape: {robot_actions.shape}")
#     assert robot_actions.shape == (N_ROBOTS, ACTION_DIM), "ë¡œë´‡ ì•¡ì…˜ Shape ë¶ˆì¼ì¹˜"

#     # --- ë‹¨ê³„ 4: ë¦¬í”Œë ˆì´ ë²„í¼ ì±„ìš°ê¸° ---
#     for _ in range(BATCH_SIZE * 2):
#         next_raw_state, reward, done, _ = env.step(robot_actions)
#         next_state_graph = convert_state_to_graph(next_raw_state)
#         replay_buffer.add(state_graph, torch.from_numpy(robot_actions).float(), next_state_graph, reward, done)
#         state_graph = next_state_graph
#         if done:
#             raw_state, _ = env.reset()
#             state_graph = convert_state_to_graph(raw_state)
#         # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ ìƒˆë¡œìš´ ì•¡ì…˜ ì„ íƒ
#         action_tensor = agent.select_action(state_graph)
#         robot_actions = action_tensor[0:N_ROBOTS]

#     print(f"âœ… [4/5] ë¦¬í”Œë ˆì´ ë²„í¼ì— ë°ì´í„° ì¶”ê°€ ì„±ê³µ (í˜„ì¬ í¬ê¸°: {len(replay_buffer)})")

#     # --- ë‹¨ê³„ 5: ì—ì´ì „íŠ¸ í›ˆë ¨ (ê°€ì¥ ì¤‘ìš”í•œ ê²€ì¦) ---
#     print("â³ [5/5] agent.train() 1íšŒ ì‹¤í–‰ ì‹œë„...")
#     try:
#         agent.train(replay_buffer, BATCH_SIZE)
#         print("âœ… [5/5] agent.train() 1íšŒ ì‹¤í–‰ ì„±ê³µ!")
#     except Exception as e:
#         print(f"âŒ [5/5] agent.train() ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ!")
#         print("\n--- ERROR ---")
#         print(e)
#         print("\n--- DEBUG INFO ---")
#         print("ì˜¤ë¥˜ë¥¼ ìœ ë°œí–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.")
#         s, a, ns, r, nd = replay_buffer.sample(BATCH_SIZE)
#         print("State Batch:", s)
#         print("Action Batch Shape:", a.shape)
#         print("Next State Batch:", ns)
#         print("Reward Batch Shape:", r.shape)
#         return

#     print("\nğŸ‰ --- íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì™„ë£Œ: í•µì‹¬ ë¡œì§ì— ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. --- ğŸ‰")

# if __name__ == "__main__":
#     run_pipeline_check()

# check_pipeline.py
import torch
import numpy as np
from torch_geometric.data import Batch

try:
    from graph_converter import (
        convert_state_to_graph, NODE_FEATURE_DIM, EDGE_FEATURE_DIM, GLOBAL_FEATURE_DIM
    )
    from replay_buffer import GraphReplayBuffer
    from agent import TD3
except ImportError as e:
    print(f"Import Error: {e}")
    exit()

class MockEnv:
    def __init__(self, n_robots=2):
        self.n_robots = n_robots
        
    def _get_obs(self):
        # graph_converterê°€ ê¸°ëŒ€í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ êµ¬ì¡° ìƒì„±
        return {
            'robot_nodes': torch.rand(1, self.n_robots, 14), # [B, N, 14]
            'current_ee_poses': torch.rand(1, self.n_robots, 7),
            'goal_poses': torch.rand(1, self.n_robots, 7), # Task node count = robot count (simplified)
            'base_poses': torch.rand(1, self.n_robots, 7),
            'target_rel_pose': torch.rand(1, 7)
        }

    def reset(self):
        return self._get_obs(), {}

    def step(self, action):
        return self._get_obs(), 1.0, False, {}

def run_pipeline_check():
    print("--- RoboBallet Pipeline Check ---")
    
    N_ROBOTS = 2
    ACTION_DIM = 7
    
    # 1. Init
    env = MockEnv(N_ROBOTS)
    rb = GraphReplayBuffer(100)
    gnn_params = {
        'node_dim': NODE_FEATURE_DIM, 
        'edge_dim': EDGE_FEATURE_DIM,
        'global_dim': GLOBAL_FEATURE_DIM, 
        'action_dim': ACTION_DIM
    }
    agent = TD3(gnn_params, max_action=1.0)
    
    # 2. Reset & Graph
    raw_obs, _ = env.reset()
    graph_state = convert_state_to_graph(raw_obs)
    print(f"âœ… Graph Converted: Nodes={graph_state.num_nodes}, Edges={graph_state.num_edges}")
    
    # 3. Action
    action = agent.select_action(graph_state)
    print(f"âœ… Action Selected: {action.shape}") # Expect (2, 7)
    
    # 4. Buffer Add
    next_obs, r, d, _ = env.step(action)
    next_graph = convert_state_to_graph(next_obs)
    rb.add(graph_state, torch.tensor(action), next_graph, r, d)
    
    # Fill buffer
    for _ in range(10):
        rb.add(graph_state, torch.tensor(action), next_graph, r, d)
        
    # 5. Train
    print("â³ Training...")
    agent.train(rb, batch_size=4)
    print("ğŸ‰ íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì™„ë£Œ")

if __name__ == "__main__":
    run_pipeline_check()