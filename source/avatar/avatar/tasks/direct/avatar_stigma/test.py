# import torch
# import argparse
# import os
# import numpy as np
# from isaaclab.app import AppLauncher

# # ---------------------------------------------------------
# # 1. Argparse ì„¤ì • (ìˆ˜ì •ë¨)
# # ---------------------------------------------------------
# parser = argparse.ArgumentParser(description="Test RoboBallet Agent")

# # [ìˆ˜ì •] ëª¨ë¸ ê²½ë¡œ ì¸ì ì¶”ê°€ (í•„ìˆ˜)
# parser.add_argument("--model_path", type=str, required=True, help="Path to saved actor model (e.g., logs/.../model_step_10000_actor)")
# # í…ŒìŠ¤íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 1ê°œ í™˜ê²½ì—ì„œ ìˆ˜í–‰
# parser.add_argument("--num_envs", type=int, default=1, help="Number of environments for testing")

# AppLauncher.add_app_launcher_args(parser)
# args_cli = parser.parse_args()

# # ì•± ì‹¤í–‰
# app_launcher = AppLauncher(args_cli)
# simulation_app = app_launcher.app

# # ---------------------------------------------------------
# # 2. ëª¨ë“ˆ ì„í¬íŠ¸ (ì•± ì‹¤í–‰ í›„)
# # ---------------------------------------------------------
# from dual_arm_transport_env2 import DualrobotEnv, DualrobotCfg
# from graph_converter import (
#     convert_state_to_graph, 
#     NODE_FEATURE_DIM, EDGE_FEATURE_DIM, GLOBAL_FEATURE_DIM
# )
# from agent import TD3

# def main():
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     print(f"ğŸš€ Testing Start on {device}")

#     # ---------------------------------------------------------
#     # 3. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
#     # ---------------------------------------------------------
#     env_cfg = DualrobotCfg()
    
#     # [ì¤‘ìš”] í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” í™˜ê²½ì„ 1ê°œë¡œ ê³ ì •í•˜ëŠ” ê²ƒì´ ì‹œê°í™”ì— ìœ ë¦¬í•¨
#     # (CLIì—ì„œ --num_envsë¥¼ ë”°ë¡œ ì£¼ì§€ ì•Šì•˜ë‹¤ë©´ 1ë¡œ ì„¤ì •)
#     env_cfg.scene.num_envs = args_cli.num_envs
    
#     # ë Œë”ë§ ì¼œê¸°
#     env = DualrobotEnv(cfg=env_cfg, render_mode="human")

#     # GNN íŒŒë¼ë¯¸í„° (train.pyì™€ ë™ì¼)
#     gnn_params = {
#         'node_dim': NODE_FEATURE_DIM,
#         'edge_dim': EDGE_FEATURE_DIM,
#         'global_dim': GLOBAL_FEATURE_DIM,
#         'action_dim': 7
#     }
    
#     # ì—ì´ì „íŠ¸ ìƒì„±
#     agent = TD3(gnn_params=gnn_params, max_action=1.0)
    
#     # ---------------------------------------------------------
#     # 4. ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
#     # ---------------------------------------------------------
#     print(f"ğŸ“‚ Loading model from: {args_cli.model_path}")
    
#     if not os.path.exists(args_cli.model_path):
#         print(f"âŒ Error: Model file not found at {args_cli.model_path}")
#         return

#     # Actor ëª¨ë¸ ë¡œë“œ
#     agent.actor.load_state_dict(torch.load(args_cli.model_path, map_location=device))
#     agent.actor.eval() # í‰ê°€ ëª¨ë“œ (Dropout ë“± ë¹„í™œì„±í™”)

#     print("âœ… Model loaded. Starting simulation loop...")
    
#     # ---------------------------------------------------------
#     # 5. í…ŒìŠ¤íŠ¸ ë£¨í”„
#     # ---------------------------------------------------------
#     while simulation_app.is_running():
#         # í™˜ê²½ ë¦¬ì…‹
#         obs_dict, _ = env.reset()
        
#         while True:
#             # (1) ê·¸ë˜í”„ ë³€í™˜
#             # Test ëª¨ë“œì—ì„œëŠ” ì£¼ë¡œ 1ê°œ í™˜ê²½ì´ë¯€ë¡œ ì§ì ‘ ë³€í™˜
#             # obs_dict['policy']ì˜ ê° í…ì„œëŠ” [Num_Envs, ...] í˜•íƒœì„
#             # 0ë²ˆ í™˜ê²½ì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì™€ì„œ ê·¸ë˜í”„ë¡œ ë³€í™˜
            
#             # ë§Œì•½ num_envs > 1ì´ë¼ë©´ train.pyì²˜ëŸ¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì•¼ê² ì§€ë§Œ,
#             # ì—¬ê¸°ì„œëŠ” ì‹œê°í™”ë¥¼ ìœ„í•´ 0ë²ˆ í™˜ê²½ë§Œ ì œì–´í•˜ê±°ë‚˜, ëª¨ë“  í™˜ê²½ì„ ì œì–´í•˜ë˜ 
#             # convert_batch_obs_to_graph_list ë¡œì§ì„ ê°€ì ¸ì™€ì•¼ í•¨.
            
#             # ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•˜ê¸° ìœ„í•´ 'ëª¨ë“  í™˜ê²½'ì„ ì²˜ë¦¬í•˜ëŠ” train.py ë°©ì‹ì„ ì°¨ìš©
#             graph_list = []
#             keys = list(obs_dict['policy'].keys())
#             for i in range(env.num_envs):
#                 single_env_obs = {k: obs_dict['policy'][k][i] for k in keys}
#                 graph_list.append(convert_state_to_graph(single_env_obs))
            
#             # ë°°ì¹˜ ê·¸ë˜í”„ ìƒì„±
#             from torch_geometric.data import Batch
#             batch_graph = Batch.from_data_list(graph_list).to(device)

#             with torch.no_grad():
#                 actions_tensor = agent.actor(batch_graph) # Output: [Total_Nodes, 7]
                
#                 # --- [ìˆ˜ì • ì‹œì‘] ---
#                 # GNNì€ ë¡œë´‡ ë…¸ë“œì™€ íƒœìŠ¤í¬ ë…¸ë“œ ëª¨ë‘ì— ëŒ€í•´ ê°’ì„ ì¶œë ¥í•˜ë¯€ë¡œ,
#                 # ë¡œë´‡ ë…¸ë“œ(ì•ìª½ 2ê°œ)ë§Œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.
                
#                 num_robots = 2  # í˜„ì¬ í™˜ê²½ì˜ ë¡œë´‡ ìˆ˜
#                 action_dim = 7  # ë¡œë´‡ ë‹¹ ì•¡ì…˜ ì°¨ì›
                
#                 # 1. ë°°ì¹˜ ì°¨ì› ë³µì›: [Batch_Size, Num_Nodes_Per_Graph, Action_Dim]
#                 # actions_tensor.shape[0]ëŠ” (Batch * Num_Nodes)ì…ë‹ˆë‹¤.
#                 # í˜„ì¬ Batch=1ì´ë¯€ë¡œ Num_Nodes=4 (Robot 2 + Task 2)ê°€ ë©ë‹ˆë‹¤.
#                 num_nodes_per_graph = actions_tensor.shape[0] // env.num_envs
#                 actions_reshaped = actions_tensor.view(env.num_envs, num_nodes_per_graph, action_dim)
                
#                 # 2. ë¡œë´‡ ë…¸ë“œë§Œ ì¶”ì¶œ (graph_converterì—ì„œ ë¡œë´‡ ë…¸ë“œë¥¼ ì•ìª½ì— ë°°ì¹˜í–ˆìŒ)
#                 robot_actions = actions_reshaped[:, :num_robots, :] # [Batch, 2, 7]
                
#                 # 3. í™˜ê²½ ì…ë ¥ í˜•íƒœì¸ [Batch, 14]ë¡œ ë³€í™˜
#                 env_actions_tensor = robot_actions.reshape(env.num_envs, -1)
#                 # --- [ìˆ˜ì • ë] ---

#                 # Test ì‹œì—ëŠ” íƒí—˜ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ! (Pure Policy)
#                 # env_actions_tensor = env_actions_tensor.clamp(-1.0, 1.0)

#             # (3) í™˜ê²½ ìŠ¤í…- env.stepì€ 5ê°œì˜ ë°˜í™˜ì„ ê°–ëŠ”ë‹¤.
#             obs_dict, rewards, terminated, truncated, extras = env.step(env_actions_tensor)
#             dones = terminated | truncated #(terminated: ì„±ê³µ/ì‹¤íŒ¨ë¡œ ëë‚¨, truncated: ì‹œê°„ ì´ˆê³¼)
#             # (4) ì¢…ë£Œ í™•ì¸
#             # if dones.any():
#             #     print(f"Episode Finished. Reward: {torch.mean(rewards).item():.4f}")
#             #     # ì—”í„°í‚¤ë¥¼ ëˆ„ë¥´ë©´ ë‹¤ìŒ ì—í”¼ì†Œë“œ, ì•„ë‹ˆë©´ ê·¸ëƒ¥ ê³„ì† ì§„í–‰ ë“±
#             #     # ì—¬ê¸°ì„œëŠ” ìë™ìœ¼ë¡œ ë¦¬ì…‹ë˜ë¯€ë¡œ ë£¨í”„ ê³„ì† ë”
                
#             #     # ë§Œì•½ í•œ ì—í”¼ì†Œë“œë§Œ ë³´ê³  ì‹¶ë‹¤ë©´ break
#             #     # break
#             # (4) ì¢…ë£Œ í™•ì¸ ë° ë¡œê·¸ ì¶œë ¥
#             if dones.any():
#                 # í…ŒìŠ¤íŠ¸ëŠ” ë³´í†µ 1ê°œ í™˜ê²½(env 0)ì—ì„œ ì§„í–‰í•˜ë¯€ë¡œ 0ë²ˆ ì¸ë±ìŠ¤ ê¸°ì¤€
#                 # (ì—¬ëŸ¬ í™˜ê²½ì´ë©´ ë°˜ë³µë¬¸ ì‚¬ìš©)
#                 env_idx = 0
                
#                 # Extrasì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (0.5ë³´ë‹¤ í¬ë©´ True)
#                 is_reached = extras["log/is_reached"][env_idx].item() > 0.5
#                 is_violated = extras["log/violation"][env_idx].item() > 0.5
#                 is_success = extras["log/success"][env_idx].item() > 0.5
#                 final_reward = rewards[env_idx].item()

#                 # ìƒí™©ë³„ ë©”ì‹œì§€ ê²°ì •
#                 if is_success:
#                     status_icon = "ğŸ†"
#                     status_msg = "Perfect Success (Reached & Safe)"
#                 elif is_reached and is_violated:
#                     status_icon = "âš ï¸"
#                     status_msg = "Reached but Violated (Stigma)"
#                 elif not is_reached and is_violated:
#                     status_icon = "âŒ"
#                     status_msg = "Failed (Violated & Not Reached)"
#                 else: # not reached, not violated
#                     status_icon = "â³"
#                     status_msg = "Time Out (Safe but Not Reached)"

#                 print("-" * 50)
#                 print(f"Episode Finished!")
#                 print(f"Total Reward : {final_reward:.4f}")
#                 print(f"Status       : {status_icon} {status_msg}")
#                 print(f"Details      : Reached={is_reached}, Violated={is_violated}")
#                 print("-" * 50)

#     env.close()
#     simulation_app.close()

# if __name__ == "__main__":
#     main()

import torch
import argparse
import os
import numpy as np
from isaaclab.app import AppLauncher

# ---------------------------------------------------------
# 1. Argparse ì„¤ì • (ìˆ˜ì •ë¨)
# ---------------------------------------------------------
parser = argparse.ArgumentParser(description="Test RoboBallet Agent")

# [ìˆ˜ì •] ëª¨ë¸ ê²½ë¡œ ì¸ì ì¶”ê°€ (í•„ìˆ˜)
parser.add_argument("--model_path", type=str, required=True, help="Path to saved actor model (e.g., logs/.../model_step_10000_actor)")
# í…ŒìŠ¤íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 1ê°œ í™˜ê²½ì—ì„œ ìˆ˜í–‰
parser.add_argument("--num_envs", type=int, default=1, help="Number of environments for testing")

AppLauncher.add_app_launcher_args(parser)
args_cli = parser.parse_args()

# ì•± ì‹¤í–‰
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# ---------------------------------------------------------
# 2. ëª¨ë“ˆ ì„í¬íŠ¸ (ì•± ì‹¤í–‰ í›„)
# ---------------------------------------------------------
from dual_arm_transport_env2 import DualrobotEnv, DualrobotCfg
from graph_converter import (
    convert_state_to_graph, 
    NODE_FEATURE_DIM, EDGE_FEATURE_DIM, GLOBAL_FEATURE_DIM
)
from agent import TD3

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Testing Start on {device}")

    # ---------------------------------------------------------
    # 3. í™˜ê²½ ë° ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    # ---------------------------------------------------------
    env_cfg = DualrobotCfg()
    
    # [ì¤‘ìš”] í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” í™˜ê²½ì„ 1ê°œë¡œ ê³ ì •í•˜ëŠ” ê²ƒì´ ì‹œê°í™”ì— ìœ ë¦¬í•¨
    # (CLIì—ì„œ --num_envsë¥¼ ë”°ë¡œ ì£¼ì§€ ì•Šì•˜ë‹¤ë©´ 1ë¡œ ì„¤ì •)
    env_cfg.scene.num_envs = args_cli.num_envs
    
    # ë Œë”ë§ ì¼œê¸°
    env = DualrobotEnv(cfg=env_cfg, render_mode="human")

    # GNN íŒŒë¼ë¯¸í„° (train.pyì™€ ë™ì¼)
    gnn_params = {
        'node_dim': NODE_FEATURE_DIM,
        'edge_dim': EDGE_FEATURE_DIM,
        'global_dim': GLOBAL_FEATURE_DIM,
        'action_dim': 7
    }
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = TD3(gnn_params=gnn_params, max_action=1.0)
    
    # ---------------------------------------------------------
    # 4. ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    # ---------------------------------------------------------
    print(f"ğŸ“‚ Loading model from: {args_cli.model_path}")
    
    if not os.path.exists(args_cli.model_path):
        print(f"âŒ Error: Model file not found at {args_cli.model_path}")
        return

    # Actor ëª¨ë¸ ë¡œë“œ
    agent.actor.load_state_dict(torch.load(args_cli.model_path, map_location=device))
    agent.actor.eval() # í‰ê°€ ëª¨ë“œ (Dropout ë“± ë¹„í™œì„±í™”)

    print("âœ… Model loaded. Starting simulation loop...")
    
    # ---------------------------------------------------------
    # 5. í…ŒìŠ¤íŠ¸ ë£¨í”„
    # ---------------------------------------------------------
    while simulation_app.is_running():
        # --- ìƒˆ ì—í”¼ì†Œë“œë¥¼ ìœ„í•œ ë¦¬ì…‹ ---
        obs_dict, _ = env.reset()
        
        # [NEW] ìœ„ë°˜ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
        is_in_violation = [False] * env.num_envs
        violation_start_step = [-1] * env.num_envs
        step_counter = 0
        
        print("\n" + "="*60)
        print("Starting New Test Episode...")
        print("="*60)


        # --- ì—í”¼ì†Œë“œ ìŠ¤í… ë£¨í”„ ---
        while True:
            # ìŠ¤í… ì¹´ìš´í„° ì¦ê°€
            step_counter += 1
            
            # (1) ê·¸ë˜í”„ ë³€í™˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            graph_list = []
            keys = list(obs_dict['policy'].keys())
            for i in range(env.num_envs):
                single_env_obs = {k: obs_dict['policy'][k][i] for k in keys}
                graph_list.append(convert_state_to_graph(single_env_obs))
            
            from torch_geometric.data import Batch
            batch_graph = Batch.from_data_list(graph_list).to(device)

            with torch.no_grad():
                # (2) ì•¡ì…˜ ì¶”ë¡  (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                actions_tensor = agent.actor(batch_graph)
                num_nodes_per_graph = actions_tensor.shape[0] // env.num_envs
                actions_reshaped = actions_tensor.view(env.num_envs, num_nodes_per_graph, 7)
                robot_actions = actions_reshaped[:, :2, :]
                env_actions_tensor = robot_actions.reshape(env.num_envs, -1)

            # (3) í™˜ê²½ ìŠ¤í…
            obs_dict, rewards, terminated, truncated, extras = env.step(env_actions_tensor)
            
            # [NEW] ìœ„ë°˜ ë¡œê¹… ë¡œì§
            currently_violating_tensor = extras["log/is_currently_violated"]
            for i in range(env.num_envs):
                is_currently_violated = currently_violating_tensor[i].item()

                # Case 1: ìœ„ë°˜ ì‹œì‘
                if is_currently_violated and not is_in_violation[i]:
                    is_in_violation[i] = True
                    violation_start_step[i] = step_counter
                    print(f"ğŸ”´ [Env {i}, Step {step_counter}] Constraint violation STARTED.")
                
                # Case 2: ìœ„ë°˜ ì¢…ë£Œ
                elif not is_currently_violated and is_in_violation[i]:
                    print(f"ğŸŸ¢ [Env {i}] Constraint violation ENDED. (Duration: {violation_start_step[i]} ~ {step_counter - 1})")
                    is_in_violation[i] = False
                    violation_start_step[i] = -1
            
            # (4) ì¢…ë£Œ í™•ì¸
            dones = terminated | truncated
            if dones.any():
                # [NEW] ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ, ì§„í–‰ ì¤‘ì´ë˜ ìœ„ë°˜ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸
                for i in range(env.num_envs):
                    if dones[i] and is_in_violation[i]:
                         print(f"ğŸŸ¡ [Env {i}] Episode ended while in violation. (Started at step {violation_start_step[i]})")

                # ìµœì¢… ê²°ê³¼ ì¶œë ¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                env_idx = 0 # 1ê°œ í™˜ê²½ ê¸°ì¤€
                is_success = extras["log/success"][env_idx].item() > 0.5
                is_reached = "log/is_reached" in extras and extras["log/is_reached"][env_idx].item() > 0.5
                is_violated_final = extras["log/violation"][env_idx].item() > 0.5
                final_reward = rewards[env_idx].item()

                if is_success:
                    status_icon, status_msg = "ğŸ†", "Perfect Success (Reached & Safe)"
                elif is_reached and is_violated_final:
                    status_icon, status_msg = "âš ï¸", "Reached but Violated"
                elif not is_reached and is_violated_final:
                    status_icon, status_msg = "âŒ", "Failed (Violated & Not Reached)"
                else:
                    status_icon, status_msg = "â³", "Time Out (Safe but Not Reached)"

                print("-" * 60)
                print(f"Episode Finished at step {step_counter}!")
                print(f"Total Reward : {final_reward:.4f}")
                print(f"Status       : {status_icon} {status_msg}")
                print(f"Details      : Reached={is_reached}, Final Violation Status={is_violated_final}")
                print("-" * 60)
                
                # [NEW] ì•ˆìª½ ë£¨í”„ë¥¼ íƒˆì¶œí•˜ì—¬ ìƒˆ ì—í”¼ì†Œë“œì—ì„œ ë‹¤ì‹œ ì‹œì‘í•˜ë„ë¡ í•¨
                break
    
    env.close()
    simulation_app.close()

if __name__ == "__main__":
    main()