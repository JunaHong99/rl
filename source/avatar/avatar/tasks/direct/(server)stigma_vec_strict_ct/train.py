# train.py

import torch
import numpy as np
import os
import argparse
from datetime import datetime
from collections import deque
from torch_geometric.data import Batch
from torch.utils.tensorboard import SummaryWriter

# Isaac Lab Imports
from isaaclab.app import AppLauncher

# [ì¤‘ìš”] argparse ì„¤ì •ì„ AppLauncherë³´ë‹¤ ë¨¼ì € í•´ì•¼ í•¨
parser = argparse.ArgumentParser(description="Train RoboBallet Agent")
# í•™ìŠµ ì†ë„ì™€ ì•ˆì •ì„±ì„ ìœ„í•´ í™˜ê²½ ìˆ˜ì™€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ëŠ˜ë ¸ìŠµë‹ˆë‹¤.
parser.add_argument("--num_envs", type=int, default=1, help="Number of parallel environments")
parser.add_argument("--max_iterations", type=int, default=500000, help="Total training iterations")
parser.add_argument("--resume_path", type=str, default=None, help="Path to checkpoint prefix (e.g. logs/.../model_step_50000)")
AppLauncher.add_app_launcher_args(parser)
args_cli = parser.parse_args()

# ì•± ì‹¤í–‰
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# ë‚˜ë¨¸ì§€ ëª¨ë“ˆ ì„í¬íŠ¸ (App ì‹¤í–‰ í›„)
from dual_arm_transport_env3 import DualrobotEnv, DualrobotCfg
from graph_converter import (
    convert_batch_state_to_graph,
    NODE_FEATURE_DIM, EDGE_FEATURE_DIM, GLOBAL_FEATURE_DIM
)
from replay_buffer import VectorizedGraphReplayBuffer
from agent import TD3

# ---------------------------------------------------------
# 2. Main Training Loop
# ---------------------------------------------------------
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Training Start on {device}")

    # --- A. í™˜ê²½ ë° ë¡œê¹… ì´ˆê¸°í™” ---
    run_name = f"roboballet_{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    script_dir = os.path.dirname(os.path.abspath(__file__))  # train.pyê°€ ìˆëŠ” í´ë”
    #log_dir = os.path.join(script_dir, "logs", run_name)     # ê·¸ ì•ˆì— logs ìƒì„±
    if args_cli.resume_path:
        # ì˜ˆ: args_cli.resume_path = ".../logs/roboballet_2025.../model_step_50000"
        # os.path.dirnameì„ í•˜ë©´ ".../logs/roboballet_2025..." í´ë” ê²½ë¡œê°€ ë‚˜ì˜µë‹ˆë‹¤.
        log_dir = os.path.dirname(args_cli.resume_path)
        print(f"ğŸ“‚ Resuming logging into EXISTING directory: {log_dir}")
    else:
        # ê¸°ì¡´ ë¡œì§ (ìƒˆ í´ë” ìƒì„±)
        run_name = f"roboballet_{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        log_dir = os.path.join(script_dir, "logs", run_name)
        print(f"ğŸ“‚ Creating NEW log directory: {log_dir}")
    writer = SummaryWriter(log_dir)
    
    # ì„±ê³µë¥  ê³„ì‚°ì„ ìœ„í•œ ì´ë™ í‰ê·  ë²„í¼ -> ì—í”¼ì†Œë“œê°€ ëë‚  ë•Œë§Œ ì´ ë²„í¼ì— ì €ì¥ë¨. ì´ stats_buffer_size ê°œì˜ ì—í”¼ì†Œë“œë¥¼ ì €ì¥
    # ìµœì†Œ 2000ê°œ, í˜¹ì€ í™˜ê²½ ìˆ˜ì˜ 5ë°° ì¤‘ í° ê°’ìœ¼ë¡œ ì„¤ì •
    stats_buffer_size = max(2000, args_cli.num_envs * 5)
    success_buffer = deque(maxlen=stats_buffer_size)

    env_cfg = DualrobotCfg()
    env_cfg.scene.num_envs = args_cli.num_envs
    env = DualrobotEnv(cfg=env_cfg, render_mode=None) # í•™ìŠµìš©ì´ë¼ ë Œë”ë§ ë”, "human"

    # --- B. ì—ì´ì „íŠ¸ ë° ë²„í¼ ì´ˆê¸°í™” ---
    gnn_params = {
        'node_dim': NODE_FEATURE_DIM,     # 14
        'edge_dim': EDGE_FEATURE_DIM,     # 9
        'global_dim': GLOBAL_FEATURE_DIM, # 19
        'action_dim': 7                   # ë¡œë´‡ 1ëŒ€ë‹¹ ì•¡ì…˜
    }
    
    # í•™ìŠµë¥  ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    agent = TD3(gnn_params=gnn_params, max_action=1.0, lr=3e-4)
    
    # --- C. í•™ìŠµ ë£¨í”„ ì‹œì‘ ---
    obs_dict, _ = env.reset()
    
    # [Vectorized] ì´ˆê¸° ê·¸ë˜í”„ ìƒì„± (GPU ìœ ì§€)
    current_batch_graph = convert_batch_state_to_graph(obs_dict['policy'], args_cli.num_envs)
    
    # [NEW] Vectorized Buffer ì´ˆê¸°í™” (í…œí”Œë¦¿ ê·¸ë˜í”„ í•„ìš”í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì´ˆê¸°í™”)
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì£¼ì˜: (1M * Nodes * Dim * 4bytes) -> 1M * 4 * 14 * 4 ~= 224MB (ë§¤ìš° ì‘ìŒ)
    # GPU ë©”ëª¨ë¦¬ê°€ ë„‰ë„‰í•˜ë‹¤ë©´ device="cuda" ê¶Œì¥
    buffer = VectorizedGraphReplayBuffer(
        capacity=40000000, 
        num_envs=args_cli.num_envs,
        node_dim=NODE_FEATURE_DIM,
        edge_dim=EDGE_FEATURE_DIM,
        global_dim=GLOBAL_FEATURE_DIM,
        action_dim=14, # 2 Robots * 7
        template_graph=current_batch_graph,
        device="cuda" # GPU ì €ì¥
    )

    start_step = 0  # ê¸°ë³¸ ì‹œì‘ ìŠ¤í…

    if args_cli.resume_path is not None:
        if os.path.exists(args_cli.resume_path + "_actor"): # íŒŒì¼ ì¡´ì¬ í™•ì¸
            print(f"ğŸ”„ Resuming training from: {args_cli.resume_path}")
            
            # 1. ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
            agent.load(args_cli.resume_path)
            
            # 2. íŒŒì¼ëª…ì—ì„œ ìŠ¤í… ìˆ˜ ì¶”ì¶œ (ì˜ˆ: "model_step_50000" -> 50000)
            try:
                # ê²½ë¡œì˜ ë§¨ ë’¤ íŒŒì¼ëª…ë§Œ ê°€ì ¸ì˜´ -> '_'ë¡œ ë¶„ë¦¬ -> ë§ˆì§€ë§‰ ìˆ«ì íŒŒì‹±
                filename = os.path.basename(args_cli.resume_path) 
                start_step = int(filename.split('_')[-1])
                print(f"â© Start Step updated to: {start_step}")
            except Exception as e:
                print(f"âš ï¸ Could not parse step from filename. Starting from 0. Error: {e}")
        else:
            print(f"âŒ Checkpoint not found at {args_cli.resume_path}. Starting from scratch.")

    print(f"ğŸ”„ Start Interaction Loop ({args_cli.max_iterations} steps)...")
    print(f"ğŸ“‚ Logs will be saved to: {log_dir}")

    MAX_EPISODE_STEPS = 300
    WARMUP_STEPS = MAX_EPISODE_STEPS *2

    for step in range(start_step, args_cli.max_iterations):
        
        # -------------------------------------------------
        # 1. Action Selection (GNN Inference)
        # -------------------------------------------------
        # [Vectorized] ì´ë¯¸ Batch ê°ì²´ì´ë¯€ë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
        if step < WARMUP_STEPS:
            # -0.5 ~ 0.5 ì‚¬ì´ì˜ ê· ë“± ë¶„í¬ ëœë¤ ì•¡ì…˜ (ê³„ì‚° ë¹„ìš© 0)
            env_actions_tensor = 1 * torch.rand(args_cli.num_envs, 14, device=device) - 0.5
        else:   
            agent.actor.eval()
            with torch.no_grad():
                # GNN ì¶œë ¥: [Total_Nodes, 7] 
                full_actions = agent.actor(current_batch_graph)
                
                # [ìˆ˜ì • ì œì•ˆ: ë™ì  ê³„ì‚°] -----------------------------------------
                # 1. ì „ì²´ ë…¸ë“œ ìˆ˜ì™€ í™˜ê²½ ìˆ˜ë¡œ 'ê·¸ë˜í”„ë‹¹ ë…¸ë“œ ìˆ˜'ë¥¼ ì—­ì‚°í•©ë‹ˆë‹¤.
                #    ì´ë ‡ê²Œ í•˜ë©´ ì¥ì• ë¬¼ì´ë‚˜ íƒœìŠ¤í¬ê°€ ëŠ˜ì–´ë‚˜ì„œ ë…¸ë“œ ìˆ˜ê°€ 4ê°œê°€ ì•„ë‹ˆê²Œ ë˜ì–´ë„ ì½”ë“œê°€ ì‘ë™í•©ë‹ˆë‹¤.
                total_nodes = full_actions.shape[0]
                num_envs = args_cli.num_envs
                
                # ì‚°ìˆ  ê²€ì¦ (Total NodesëŠ” ë°˜ë“œì‹œ Num Envsì˜ ë°°ìˆ˜ì—¬ì•¼ í•¨)
                assert total_nodes % num_envs == 0, f"Node mismatch: {total_nodes} nodes for {num_envs} envs"
                
                num_nodes_per_env = total_nodes // num_envs  # ì˜ˆ: 4, 5, 6... ë“±ìœ¼ë¡œ ìë™ ê³„ì‚°ë¨

                # 2. [Num_Envs, Node_Per_Env, Action_Dim] í˜•íƒœë¡œ ë³€í™˜
                reshaped_actions = full_actions.view(num_envs, num_nodes_per_env, -1)
                
                # 3. ë¡œë´‡ ë…¸ë“œë§Œ ìŠ¬ë¼ì´ì‹±
                # (ì£¼ì˜: DualArm í™˜ê²½ì´ë¯€ë¡œ ë¡œë´‡ì€ í•­ìƒ 2ëŒ€ë¼ê³  ê°€ì •í•˜ê±°ë‚˜, 
                #  env ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ë³€ìˆ˜(env.num_robots ë“±)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤)
                num_robots = 2 
                
                # graph_converterì—ì„œ ë¡œë´‡ ë…¸ë“œë¥¼ 0, 1ë²ˆ ì¸ë±ìŠ¤ì— ë„£ì—ˆìœ¼ë¯€ë¡œ ì•ë¶€ë¶„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
                robot_actions = reshaped_actions[:, :num_robots, :] # [Num_Envs, 2, 7]
                
                # 4. í™˜ê²½ ì…ë ¥ìš© í”Œë˜íŠ¼ [Num_Envs, 14]
                env_actions_tensor = robot_actions.reshape(num_envs, -1)
                # -------------------------------------------------------------
                
                # Exploration Noise ì¶”ê°€
                noise = torch.randn_like(env_actions_tensor) * 0.1
                env_actions_tensor = (env_actions_tensor + noise).clamp(-1.0, 1.0)

            agent.actor.train()

        # -------------------------------------------------
        # 2. Environment Step
        # -------------------------------------------------
        next_obs_dict, rewards, terminated, truncated, extras = env.step(env_actions_tensor)
        
        # ë¦¬í”Œë ˆì´ ë²„í¼ ì €ì¥ì„ ìœ„í•´ terminatedì™€ truncatedë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ doneìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        dones = terminated | truncated
        
        # -------------------------------------------------
        # 3. Data Handling (Convert & Buffer)
        # -------------------------------------------------
        # [Vectorized] Next State ë³€í™˜ (GPU ìœ ì§€)
        next_batch_graph = convert_batch_state_to_graph(next_obs_dict['policy'], args_cli.num_envs)
        
        # [Buffer ì €ì¥] Vectorized Bufferì— GPU Tensor ê·¸ëŒ€ë¡œ íˆ¬ì… (ë§¤ìš° ë¹ ë¦„)
        buffer.add_batch(
            state_batch=current_batch_graph, 
            action=env_actions_tensor,
            next_state_batch=next_batch_graph,
            reward=rewards,
            done=dones
        )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸ (GPU ê°ì²´ ê·¸ëŒ€ë¡œ ë„˜ê¹€)
        current_batch_graph = next_batch_graph
        
        # -------------------------------------------------
        # 4. Train Agent
        # -------------------------------------------------
        # ë²„í¼ê°€ ì–´ëŠ ì •ë„ ì°¨ë©´ í•™ìŠµ ì‹œì‘ (ë°°ì¹˜ ì‚¬ì´ì¦ˆ 256 ê¶Œì¥)
        # [ìˆ˜ì •] 4096 í™˜ê²½ ë“± ëŒ€ê·œëª¨ ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ë°ì´í„°ê°€ ë¹¨ë¦¬ ì°¨ë¯€ë¡œ ì›Œë°ì—…ì„ ì¤„ì´ê³ , ì—…ë°ì´íŠ¸ íšŸìˆ˜ë¥¼ ëŠ˜ë¦¼
        if step >= WARMUP_STEPS:
            gradient_steps = max(1, args_cli.num_envs // 128)
            for _ in range(gradient_steps): #ë²„í¼ì—ì„œ 256ê°œì˜ ë°ì´í„°ë¥¼ ë½‘ì•„ ì—…ë°ì´íŠ¸, ì´ê±¸ ê·¸ë¼ë””ì–¸íŠ¸ ìŠ¤í…ë§Œí¼ ë°˜ë³µ.
                agent.train(buffer, batch_size=256)

        # -------------------------------------------------
        # 5. Logging (TensorBoard)
        # -------------------------------------------------
        # ì„±ê³µë¥  ì§‘ê³„ (ëë‚œ ì—í”¼ì†Œë“œê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        if dones.any():
            # extrasì—ì„œ ì„±ê³µ ì—¬ë¶€ ê°€ì ¸ì˜¤ê¸° ('log/success' í‚¤ê°€ ìˆë‹¤ê³  ê°€ì •)
            done_indices = dones.nonzero(as_tuple=False).squeeze(-1)
            
            if "log/success" in extras:
                success_vals = extras["log/success"][done_indices].cpu().numpy()
                success_buffer.extend(success_vals)

        # ì£¼ê¸°ì  ê¸°ë¡ (ë§¤ 100 ìŠ¤í…)
        if step % 100 == 0:
            mean_reward = torch.mean(rewards).item()

            # (2) ë¦¬ì›Œë“œ ì„±ë¶„ë³„ ê¸°ë¡ (ìˆ˜ì •ë¨)
            # extrasì— í‚¤ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ê¸°ë¡
            if "log/total_reward" in extras:
                writer.add_scalar("Reward/Constraint", extras["log/r_constraint"].item(), step)
                writer.add_scalar("Reward/Action", extras["log/r_action"].item(), step)

            # [CHANGED] Log potential reward instead of absolute distance reward
            if "log/r_potential" in extras:
                writer.add_scalar("Reward/Potential", extras["log/r_potential"].item(), step)
            elif "log/r_dist" in extras: # Fallback if using old env
                writer.add_scalar("Reward/Distance", extras["log/r_dist"].item(), step)


            # (3) ë””ë²„ê¹…ìš© ì—ëŸ¬ ê¸°ë¡
            if "log/err_pos" in extras:
                writer.add_scalar("Error/position", extras["log/err_pos"].item(), step)
                writer.add_scalar("Error/rotation", extras["log/err_rot"].item(), step)
            
            # [NEW] ì¶”ê°€ëœ ì„±ëŠ¥ ì§€í‘œ ë¡œê¹…
            if "log/max_err_pos" in extras:
                # Note: This logs the MEAN of the "Episode Max Errors" across envs.
                # Ideally, to detect if ANY violation occurred, we check if this value > threshold.
                writer.add_scalar("Error/Max_Position", extras["log/max_err_pos"].item(), step)
                writer.add_scalar("Error/Max_Rotation", extras["log/max_err_rot"].item(), step)
            if "log/violation_ratio" in extras:
                # This is the Ratio of envs currently violating. 0.0 means PERFECT safety.
                writer.add_scalar("Rollout/ViolationRatio", extras["log/violation_ratio"].item(), step)

            # (4) ì„±ê³µë¥  ê¸°ë¡
            if len(success_buffer) > 0:
                success_rate = np.mean(success_buffer)
                writer.add_scalar("Rollout/SuccessRate", success_rate, step)
            else:
                success_rate = 0.0

            # ì½˜ì†” ì¶œë ¥
            print(f"[Step {step}/{args_cli.max_iterations}] "
                  f"Rew: {mean_reward:.4f} | "
                  f"Succ: {success_rate:.1%} | "
                  f"Buff: {len(buffer)}")
            
            # ëª¨ë¸ ì €ì¥ (ë§¤ 5000 ìŠ¤í…)
            if step % 5000 == 0:
                save_path = os.path.join(log_dir, f"model_step_{step}")
                agent.save(save_path)

    print("âœ… Training Finished!")
    writer.close()
    env.close()
    simulation_app.close()

if __name__ == "__main__":
    # ì‹¤í–‰ ì‹œ ì¸ìë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŒ (ì˜ˆ: python train.py --num_envs 512)
    main()